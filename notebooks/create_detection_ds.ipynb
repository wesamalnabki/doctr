{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from pdf2image import convert_from_bytes\n",
    "from PIL import ImageDraw\n",
    "import uuid \n",
    "import os \n",
    "import json \n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "detection_root_images = r\"/opt/walnabki/OCR_Dataset/detection_ds/images\"\n",
    "os.makedirs(detection_root_images, exist_ok=True)\n",
    "detection_root_labels = r\"/opt/walnabki/OCR_Dataset/detection_ds/labels\"\n",
    "os.makedirs(detection_root_labels, exist_ok=True)\n",
    "\n",
    "\n",
    "detct_train_root_images = r\"/opt/walnabki/OCR_Dataset/detection_train/images\"\n",
    "os.makedirs(detct_train_root_images, exist_ok=True)\n",
    "detct_train_root_labels = r\"/opt/walnabki/OCR_Dataset/detection_train/labels\"\n",
    "os.makedirs(detct_train_root_labels, exist_ok=True)\n",
    "\n",
    "train_json_path = r'/opt/walnabki/OCR_Dataset/detection_train/labels.json'\n",
    "\n",
    "detct_val_root_images = r\"/opt/walnabki/OCR_Dataset/detection_val/images\"\n",
    "os.makedirs(detct_val_root_images, exist_ok=True)\n",
    "detct_val_root_labels = r\"/opt/walnabki/OCR_Dataset/detection_val/labels\"\n",
    "os.makedirs(detct_val_root_labels, exist_ok=True)\n",
    "\n",
    "val_json_path = r'/opt/walnabki/OCR_Dataset/detection_val/labels.json'\n",
    "\n",
    "dataset = load_dataset('pixparse/pdfa-eng-wds', streaming=True )\n",
    "\n",
    "VAL_SIZE = 0.02\n",
    "MAX_PAGES_COUNT =  100 # None to ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_cord(bbox, img_w, img_h):\n",
    "    x, y, w, h = bbox\n",
    "    x1, y1 = x, y\n",
    "    x2, y2 = x + w, y\n",
    "    x3, y3 = x + w, y + h\n",
    "    x4, y4 = x, y + h\n",
    "\n",
    "    return [\n",
    "        [int(x1 * img_w), int(y1 * img_h)],\n",
    "        [int(x2 * img_w), int(y2 * img_h)],\n",
    "        [int(x3 * img_w), int(y3 * img_h)],\n",
    "        [int(x4 * img_w), int(y4 * img_h)]\n",
    "    ]\n",
    "\n",
    "def build_page_input(image_file, bboxs):\n",
    "    fn = str(uuid.uuid4())\n",
    "    img_name = fn+\".png\"\n",
    "    json_name = fn+\".json\"\n",
    "    image_file.save(os.path.join(detection_root_images, img_name))\n",
    "    bboxs = [adjust_cord(bbox, image_file.size[0], image_file.size[1]) for bbox in bboxs]\n",
    "    data = {\n",
    "        fn +\".png\": {\n",
    "        'img_dimensions': image_file.size,\n",
    "        'img_hash': fn,\n",
    "        'polygons': bboxs\n",
    "     }\n",
    "    }\n",
    "    with open(os.path.join(detection_root_labels, json_name), 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start creating detection dataset\")\n",
    "ctr = 0\n",
    "\n",
    "for sample in iter(dataset['train']):\n",
    "    \n",
    "    if MAX_PAGES_COUNT and ctr > MAX_PAGES_COUNT:\n",
    "        break\n",
    "\n",
    "    pdf = sample['pdf']\n",
    "    pages  =sample['json']['pages']\n",
    "    pages_imgs = convert_from_bytes(pdf)\n",
    "\n",
    "    for page_id in range(len(pages_imgs)): \n",
    "\n",
    "        ctr +=1         \n",
    "        try:\n",
    "            words = pages[page_id]['words']['text']\n",
    "            bboxs = pages[page_id]['words']['bbox']\n",
    "            \n",
    "            if len(words)!=len(bboxs):\n",
    "                print(\"MASSIVE ERROR --> ignore page\")\n",
    "                continue\n",
    "\n",
    "            images_bbox  = pages[page_id]['words']['bbox']\n",
    "            page_img = pages_imgs[page_id]\n",
    "\n",
    "            if bboxs:\n",
    "                build_page_input(image_file = page_img, bboxs = bboxs)\n",
    "            else:\n",
    "                print(\"no bboxs--> escape\")\n",
    "                continue        \n",
    "        except Exception as ex:\n",
    "            print(\"some exception, escape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Split the dataset\")\n",
    "img_names = [x[:-4] for x in os.listdir(detection_root_images)]\n",
    "x_train ,x_test = train_test_split(img_names,test_size=VAL_SIZE, random_state=42) \n",
    "len(x_train), len(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Moving imgs to val folder\")\n",
    "for fn_test in tqdm(x_test):\n",
    "    src_lab = os.path.join(detection_root_labels, fn_test + \".json\") \n",
    "    des_lab = os.path.join(detct_val_root_labels, fn_test + \".json\") \n",
    "\n",
    "    src_img = os.path.join(detection_root_images, fn_test + \".png\") \n",
    "    des_img = os.path.join(detct_val_root_images, fn_test + \".png\")\n",
    "\n",
    "    shutil.move(src_lab, des_lab) \n",
    "    shutil.move(src_img, des_img)\n",
    "\n",
    "print(\"Creating labels file for training\")\n",
    "all_data_val = dict()\n",
    "for f in os.listdir(detct_val_root_labels):\n",
    "    with open(os.path.join(detct_val_root_labels , f)) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        all_data_val.update(data) \n",
    "\n",
    "\n",
    "with open(val_json_path, 'w') as outfile:\n",
    "    json.dump(all_data_val, outfile, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Moving imgs to training folder\")\n",
    "for fn_train in tqdm(x_train):\n",
    "    src_lab = os.path.join(detection_root_labels, fn_train + \".json\") \n",
    "    des_lab = os.path.join(detct_train_root_labels, fn_train + \".json\") \n",
    "\n",
    "    src_img = os.path.join(detection_root_images, fn_train + \".png\") \n",
    "    des_img = os.path.join(detct_train_root_images, fn_train + \".png\")\n",
    "\n",
    "    shutil.move(src_lab, des_lab) \n",
    "    shutil.move(src_img, des_img)\n",
    "\n",
    "print(\"Creating labels file for training\")\n",
    "all_data_train = dict()\n",
    "for f in os.listdir(detct_train_root_labels):\n",
    "    with open(os.path.join(detct_train_root_labels , f)) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        all_data_train.update(data) \n",
    "\n",
    "\n",
    "with open(train_json_path, 'w') as outfile:\n",
    "    json.dump(all_data_train, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
